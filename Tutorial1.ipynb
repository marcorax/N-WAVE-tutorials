{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed10f9f5",
   "metadata": {},
   "source": [
    "# Hardware Model 1 on QUT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f60d9f",
   "metadata": {},
   "source": [
    "### A Simple Introduction to Spiking Neural Networks with NeuronovaSim\n",
    "First, we need to import common libraries and Neuronova-Sim (N-Wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7aced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import necessary components from your library\n",
    "from Neuronova_Sim.NeuronovaSim import NeuronovaSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "212d1323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set our seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be89f22e",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "We use mini_QUT, an internal dataset composed of the first 100 recordings for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a34b13bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 classes: ['CAFE-mini', 'CAR-mini', 'HOME-mini', 'STREET-mini']\n",
      "Class 'CAFE-mini': 96 samples\n",
      "Class 'HOME-mini': 96 samples\n",
      "Class 'CAR-mini': 96 samples\n",
      "Class 'STREET-mini': 96 samples\n",
      "Using 'train' split with 308 samples\n",
      "Found 4 classes: ['CAFE-mini', 'CAR-mini', 'HOME-mini', 'STREET-mini']\n",
      "Class 'CAFE-mini': 96 samples\n",
      "Class 'HOME-mini': 96 samples\n",
      "Class 'CAR-mini': 96 samples\n",
      "Class 'STREET-mini': 96 samples\n",
      "Using 'val' split with 76 samples\n"
     ]
    }
   ],
   "source": [
    "class CochleaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for loading neural response data organized in class-specific folders.\n",
    "    Data structure:\n",
    "    root_folder/\n",
    "        class1/\n",
    "            file1.csv\n",
    "            file2.csv\n",
    "        class2/\n",
    "            file3.csv\n",
    "            file4.csv\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_folder, split=\"train\", val_split=0.2, random_state=42):\n",
    "        assert split in [\n",
    "            \"train\",\n",
    "            \"val\",\n",
    "            \"test\",\n",
    "        ], f\"Split {split} not recognized. Use 'train', 'val', or 'test'\"\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {}\n",
    "        self.split = split\n",
    "\n",
    "        # Get all class folders\n",
    "        class_folders = [\n",
    "            d\n",
    "            for d in os.listdir(root_folder)\n",
    "            if os.path.isdir(os.path.join(root_folder, d))\n",
    "        ]\n",
    "\n",
    "        # Save classes number\n",
    "        self.n_classes = len(class_folders)\n",
    "\n",
    "        # Create class mapping\n",
    "        self.class_to_idx = {\n",
    "            cls_name: idx for idx, cls_name in enumerate(sorted(class_folders))\n",
    "        }\n",
    "\n",
    "        print(f\"Found {self.n_classes} classes: {list(self.class_to_idx.keys())}\")\n",
    "\n",
    "        # Collect all files and labels\n",
    "        all_files = []\n",
    "        all_labels = []\n",
    "        for class_name in class_folders:\n",
    "            class_path = os.path.join(root_folder, class_name)\n",
    "            class_files = [\n",
    "                os.path.join(class_path, f)\n",
    "                for f in os.listdir(class_path)\n",
    "                if f.endswith(\".csv\") and not f.startswith(\".\")\n",
    "            ]\n",
    "            all_files.extend(class_files)\n",
    "            all_labels.extend([self.class_to_idx[class_name]] * len(class_files))\n",
    "            print(f\"Class '{class_name}': {len(class_files)} samples\")\n",
    "\n",
    "        # Create a deterministic split\n",
    "        generator = torch.Generator().manual_seed(random_state)\n",
    "        indices = torch.randperm(len(all_files), generator=generator)\n",
    "\n",
    "        # Calculate split sizes\n",
    "        val_size = int(len(indices) * val_split)\n",
    "        train_size = len(indices) - val_size\n",
    "\n",
    "        # Select appropriate indices based on split\n",
    "        if split == \"train\":\n",
    "            split_indices = indices[:train_size]\n",
    "        elif split == \"val\":\n",
    "            split_indices = indices[train_size:]\n",
    "        elif split == \"test\":\n",
    "            split_indices = indices\n",
    "\n",
    "        # Use the selected indices to populate the dataset\n",
    "        self.files = [all_files[i] for i in split_indices]\n",
    "        self.labels = [all_labels[i] for i in split_indices]\n",
    "\n",
    "        print(f\"Using '{split}' split with {len(self.files)} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        label = torch.tensor(label)\n",
    "        label_one_hot = torch.nn.functional.one_hot(label, num_classes=self.n_classes)\n",
    "\n",
    "        # Load neural response data\n",
    "        # Each row is a time bin, each column is a neuron\n",
    "        data = np.loadtxt(file_path, delimiter=\",\", dtype=np.float32, encoding=\"latin1\")\n",
    "\n",
    "        # Reshape to (time_bins, n_neurons)\n",
    "        # No need to add extra dimension since we already have multiple neurons\n",
    "        data = data.reshape(data.shape[0], -1)  # Force 2D shape in case single neuron\n",
    "\n",
    "        # Convert to tensor with shape (time_bins, n_neurons)\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "        return data_tensor, label_one_hot, file_path\n",
    "\n",
    "\n",
    "# Dataset disk position and batchsize\n",
    "data_dir = \"data/4bit_frontend/mini_TRAIN_VAL\"\n",
    "batch_size = 32\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = CochleaDataset(data_dir, split=\"train\")\n",
    "val_dataset = CochleaDataset(data_dir, split=\"val\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911e298d",
   "metadata": {},
   "source": [
    "### Let's define the network\n",
    "We need to normalize the dataset since it is quantized to 4 bits\n",
    "(So let's devide the input by 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354785a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "NeuronovaSim.__init__() got an unexpected keyword argument 'network_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 95\u001b[0m\n\u001b[1;32m     91\u001b[0m norm_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m  \u001b[38;5;66;03m# Used to normalize input between 0 and 1\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Create the SNN model\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleSNN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_mismatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbits_synapse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to a number to enable quantization\u001b[39;49;00m\n\u001b[1;32m    103\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# weight_range = (0.6, 2)\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# model.sim.get_layers()[0].init_weight(weight_range)\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# model.sim.get_layers()[2].init_weight(weight_range)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Print model information\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel created with input_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, hidden_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m )\n",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m, in \u001b[0;36mSimpleSNN.__init__\u001b[0;34m(self, input_size, hidden_size, output_size, norm_factor, dt, add_mismatch, bits_synapse)\u001b[0m\n\u001b[1;32m     18\u001b[0m neurons_per_layer \u001b[38;5;241m=\u001b[39m [hidden_size, hidden_size, hidden_size, output_size]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Create the NeuronovaSim instance\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Parameters: network type, model type, input size, neurons per layer\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim \u001b[38;5;241m=\u001b[39m \u001b[43mNeuronovaSim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHWModel_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneuron_number_per_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneurons_per_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated NeuronovaSim with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inputs and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(neurons_per_layer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m layers:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Layer 1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mneurons_per_layer[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m neurons\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: NeuronovaSim.__init__() got an unexpected keyword argument 'network_type'"
     ]
    }
   ],
   "source": [
    "# Define a simple model using NeuronovaSim\n",
    "class SimpleSNN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=16,\n",
    "        hidden_size=24,\n",
    "        output_size=4,\n",
    "        norm_factor=15,  # We need to normalize the dataset with 4bit integer precision\n",
    "        dt=1e-3,\n",
    "        add_mismatch=False,\n",
    "        bits_synapse=None,  # Set to a number to enable quantization\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_factor = torch.tensor(norm_factor)\n",
    "\n",
    "        # Define network architecture - neurons per layer\n",
    "        neurons_per_layer = [hidden_size, hidden_size, hidden_size, output_size]\n",
    "\n",
    "        # Create the NeuronovaSim instance\n",
    "        # Parameters: network type, model type, input size, neurons per layer\n",
    "        self.sim = NeuronovaSim(\n",
    "            chip_topoplogy == [\"FF\", \"FF\"],\n",
    "            chip_model == \"HWModel_1\",\n",
    "            input_size=input_size,\n",
    "            neuron_number_per_layer=neurons_per_layer,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Created NeuronovaSim with {input_size} inputs and {len(neurons_per_layer)} layers:\"\n",
    "        )\n",
    "        print(f\"  - Layer 1: {neurons_per_layer[0]} neurons\")\n",
    "        print(f\"  - Layer 2: {neurons_per_layer[1]} neurons\")\n",
    "        print(f\"  - Layer 3: {neurons_per_layer[2]} neurons\")\n",
    "        print(f\"  - Output: {neurons_per_layer[3]} neurons\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be (batch_size, time_steps, input_features)\n",
    "        device = x.device\n",
    "\n",
    "        # Normalize the input\n",
    "        # This is used to normalize the first layer between 0 and 1\n",
    "        # based on the maximum firing rate from the dataset\n",
    "        x_normalized = x / self.norm_factor\n",
    "\n",
    "        # Call the simulator, which returns:\n",
    "        # (cur1, cur2, cur3, cur4, spk1, spk2, spk3, spk4, mem1, mem2, mem3, mem4)\n",
    "        outs = self.sim.forward(x_normalized)\n",
    "\n",
    "        # Unpack the simulator outputs\n",
    "        (\n",
    "            spk1,\n",
    "            spk2,\n",
    "            spk3,\n",
    "            spk4,\n",
    "            mem1,\n",
    "            mem2,\n",
    "            mem3,\n",
    "            mem4,\n",
    "            cur1,\n",
    "            cur2,\n",
    "            cur3,\n",
    "            cur4,\n",
    "        ) = outs\n",
    "\n",
    "        # Rearrange the outputs from (batch, time, features) to (time, batch, features)\n",
    "        #\n",
    "        cur1 = cur1.permute(1, 0, 2)\n",
    "        cur2 = cur2.permute(1, 0, 2)\n",
    "        cur3 = cur3.permute(1, 0, 2)\n",
    "        cur4 = cur4.permute(1, 0, 2)\n",
    "\n",
    "        spk1 = spk1.permute(1, 0, 2)\n",
    "        spk2 = spk2.permute(1, 0, 2)\n",
    "        spk3 = spk3.permute(1, 0, 2)\n",
    "        spk4 = spk4.permute(1, 0, 2)\n",
    "\n",
    "        mem1 = mem1.permute(1, 0, 2)\n",
    "        mem2 = mem2.permute(1, 0, 2)\n",
    "        mem3 = mem3.permute(1, 0, 2)\n",
    "        mem4 = mem4.permute(1, 0, 2)\n",
    "\n",
    "        # Return all outputs for detailed analysis\n",
    "        return (spk1, spk2, spk3, spk4, mem1, mem2, mem3, mem4, cur1, cur2, cur3, cur4)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "input_size = 16\n",
    "hidden_size = 24\n",
    "output_size = 4\n",
    "norm_factor = 15  # Used to normalize input between 0 and 1\n",
    "\n",
    "\n",
    "# Create the SNN model\n",
    "model = SimpleSNN(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    norm_factor=norm_factor,\n",
    "    dt=1e-3,\n",
    "    add_mismatch=False,\n",
    "    bits_synapse=None,  # Set to a number to enable quantization\n",
    ")\n",
    "\n",
    "\n",
    "# weight_range = (0.6, 2)\n",
    "# model.sim.get_layers()[0].init_weight(weight_range)\n",
    "# model.sim.get_layers()[2].init_weight(weight_range)\n",
    "# model.sim.get_layers()[4].init_weight(weight_range)\n",
    "# model.sim.get_layers()[6].init_weight(weight_range)\n",
    "\n",
    "# model.sim.get_layers()[0].fake_quant.set_w_range(w_min=1.3, w_max=2)\n",
    "# model.sim.get_layers()[2].fake_quant.set_w_range(w_min=1.3, w_max=2)\n",
    "# model.sim.get_layers()[4].fake_quant.set_w_range(w_min=1.3, w_max=2)\n",
    "# model.sim.get_layers()[6].fake_quant.set_w_range(w_min=1.3, w_max=2)\n",
    "\n",
    "# Print model information\n",
    "print(\n",
    "    f\"\\nModel created with input_size={input_size}, hidden_size={hidden_size}, output_size={output_size}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8652d4",
   "metadata": {},
   "source": [
    "### Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea934845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute accuracy\n",
    "def compute_loss_and_metrics(\n",
    "    spk_rec,\n",
    "    labels,\n",
    "):\n",
    "    ### Regularizer Params ###\n",
    "    rate_offset = 0.25\n",
    "    cross_reg_weight = 1000\n",
    "\n",
    "    ### Torch Loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    activity = spk_rec.sum(dim=1)  # -> (B, C)\n",
    "\n",
    "    # extract class-indices\n",
    "    _, targets = labels.max(-1)  # -> (B,)\n",
    "\n",
    "    # cross-entropy on (B,C) vs (B,)\n",
    "    main_loss = loss_function(activity, targets)\n",
    "\n",
    "    # firing-rate regularization\n",
    "    time_steps = spk_rec.size(1)  # ⟵ use time-axis\n",
    "    avg_firing_rate = activity.mean() / time_steps\n",
    "    rate_reg = (avg_firing_rate - rate_offset).pow(2).mean()\n",
    "\n",
    "    loss = main_loss + cross_reg_weight * rate_reg\n",
    "\n",
    "    # now compute accuracy on same summed output\n",
    "    final_out = spk_rec.sum(dim=1)\n",
    "\n",
    "    _, predicted = final_out.max(-1)  # -> (B,)\n",
    "    _, targets = labels.max(-1)  # -> (B,)\n",
    "\n",
    "    correct = predicted.eq(targets).sum().item()\n",
    "    total = targets.numel()\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "    return loss, accuracy, predicted, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b82dc",
   "metadata": {},
   "source": [
    "### Train and plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883eb199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to visualize SNN activity\n",
    "def plot_snn_activity(spk1, spk2, spk3, spk4, mem1, mem2, mem3, mem4, sample_idx=0):\n",
    "    \"\"\"\n",
    "    Plots the membrane potential and spikes for each layer.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(4, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f\"SNN Activity for Sample {sample_idx}\", fontsize=14)\n",
    "\n",
    "    layer_names = [\"Layer 1\", \"Layer 2\", \"Layer 3\", \"Output Layer\"]\n",
    "    mem_arrs = [mem1, mem2, mem3, mem4]\n",
    "    spk_arrs = [spk1, spk2, spk3, spk4]\n",
    "\n",
    "    for i in range(4):\n",
    "        if mem_arrs[i] is not None:\n",
    "            # Select one neuron from each layer for visualization\n",
    "            axs[i, 0].plot(mem_arrs[i][sample_idx, :, :].cpu().detach().numpy())\n",
    "            axs[i, 0].set_title(f\"{layer_names[i]} - Membrane Potentials\")\n",
    "            axs[i, 0].set_xlabel(\"Time Step\")\n",
    "            axs[i, 0].set_ylabel(\"Vmem\")\n",
    "\n",
    "        if spk_arrs[i] is not None:\n",
    "            # Plot spike raster for a few neurons\n",
    "            spike_data = spk_arrs[i][sample_idx, :, :].cpu().detach().numpy()\n",
    "            for n in range(min(10, spike_data.shape[1])):  # Plot up to 10 neurons\n",
    "                spike_times = np.where(spike_data[:, n] > 0.5)[0]\n",
    "                axs[i, 1].scatter(\n",
    "                    spike_times, [n] * len(spike_times), marker=\"|\", color=\"black\", s=20\n",
    "                )\n",
    "\n",
    "            axs[i, 1].set_title(f\"{layer_names[i]} - Spike Raster\")\n",
    "            axs[i, 1].set_xlabel(\"Time Step\")\n",
    "            axs[i, 1].set_ylabel(\"Neuron Index\")\n",
    "            axs[i, 1].set_ylim(-0.5, min(10, spike_data.shape[1]) - 0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to train the SNN for a few epochs\n",
    "def train_snn(model, dataloader, num_epochs=5, loss_offset=0.5):\n",
    "    \"\"\"\n",
    "    Training loop for the SNN using concepts from the original experiment code.\n",
    "    \"\"\"\n",
    "    # Define optimizer (similar to the original code which used Adamax)\n",
    "    optimizer = torch.optim.Adamax(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Training statistics tracking (similar to mean_fires in the original)\n",
    "    layer_stats = {\n",
    "        \"Firing activity layer 1\": 0,\n",
    "        \"Firing activity layer 2\": 0,\n",
    "        \"Firing activity layer 3\": 0,\n",
    "        \"Firing activity layer 4\": 0,\n",
    "        \"Train Batches\": 0,\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Reset epoch statistics\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Reset layer statistics\n",
    "        for key in layer_stats:\n",
    "            layer_stats[key] = 0\n",
    "\n",
    "        for batch_idx, (inputs, labels, _) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Unpack outputs\n",
    "            spk1, spk2, spk3, spk4, mem1, mem2, mem3, mem4, cur1, cur2, cur3, cur4 = (\n",
    "                outputs\n",
    "            )\n",
    "\n",
    "            # Calculate accuracy and loss with the updated function\n",
    "            loss, accuracy, predicted, targets = compute_loss_and_metrics(spk4, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "\n",
    "            # Add gradient clipping like in the original\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track firing activities (similar to the original code)\n",
    "            with torch.no_grad():\n",
    "                layer_stats[\"Firing activity layer 1\"] += spk1.mean().item()\n",
    "                layer_stats[\"Firing activity layer 2\"] += spk2.mean().item()\n",
    "                layer_stats[\"Firing activity layer 3\"] += spk3.mean().item()\n",
    "                layer_stats[\"Firing activity layer 4\"] += spk4.mean().item()\n",
    "                layer_stats[\"Train Batches\"] += 1\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += accuracy\n",
    "            num_batches += 1\n",
    "\n",
    "            if batch_idx == 0:\n",
    "\n",
    "                if epoch % 25 == 0:\n",
    "                    # Visualize network activity for the first sample in the batch\n",
    "                    plot_snn_activity(spk1, spk2, spk3, spk4, mem1, mem2, mem3, mem4)\n",
    "\n",
    "        # Epoch summary\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_acc = total_acc / num_batches\n",
    "\n",
    "        # Calculate average firing rates\n",
    "        avg_firing_1 = (\n",
    "            layer_stats[\"Firing activity layer 1\"] / layer_stats[\"Train Batches\"]\n",
    "        )\n",
    "        avg_firing_2 = (\n",
    "            layer_stats[\"Firing activity layer 2\"] / layer_stats[\"Train Batches\"]\n",
    "        )\n",
    "        avg_firing_3 = (\n",
    "            layer_stats[\"Firing activity layer 3\"] / layer_stats[\"Train Batches\"]\n",
    "        )\n",
    "        avg_firing_4 = (\n",
    "            layer_stats[\"Firing activity layer 4\"] / layer_stats[\"Train Batches\"]\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed:\")\n",
    "        print(f\"  - Avg Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  - Avg Accuracy: {avg_acc:.4f}\")\n",
    "        print(\n",
    "            f\"  - Avg Firing Rates: L1={avg_firing_1:.4f}, L2={avg_firing_2:.4f}, L3={avg_firing_3:.4f}, L4={avg_firing_4:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Check if firing rates are too low (early stopping like in original)\n",
    "        if avg_firing_4 <= 0.001:\n",
    "            print(\n",
    "                \"Warning: Very low firing rate in output layer. Training may be unstable.\"\n",
    "            )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c57a7c",
   "metadata": {},
   "source": [
    "### Training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2421971b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Grab one batch to visualize initial network activity\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m inputs, labels, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mtrain_loader\u001b[49m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Grab one batch to visualize initial network activity\n",
    "\n",
    "inputs, labels, _ = next(iter(train_loader))\n",
    "\n",
    "print(f\"\\nInput shape: {inputs.shape}\")\n",
    "print(f\"Label shape: {labels.shape}\")\n",
    "\n",
    "# Do a forward pass\n",
    "with torch.no_grad():\n",
    "    print(\"\\nRunning initial forward pass...\")\n",
    "    outputs = model(inputs)\n",
    "    spk1, spk2, spk3, spk4, mem1, mem2, mem3, mem4, cur1, cur2, cur3, cur4 = outputs\n",
    "\n",
    "    # Print output shapes\n",
    "    print(f\"Output spike shape (layer 4): {spk4.shape}\")\n",
    "    print(f\"Output membrane shape (layer 4): {mem4.shape}\")\n",
    "\n",
    "    # Calculate initial accuracy\n",
    "    loss, accuracy, predicted, targets = compute_loss_and_metrics(spk4, labels)\n",
    "    print(f\"Initial accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Visualize network activity\n",
    "    plot_snn_activity(spk1, spk2, spk3, spk4, mem1, mem2, mem3, mem4)\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining the SNN model...\")\n",
    "trained_model = train_snn(model, train_loader, num_epochs=100)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "\n",
    "# Test the trained model on validation set\n",
    "print(\"\\nEvaluating the trained model on validation set...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_acc = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, labels, _ in val_loader:\n",
    "        outputs = trained_model(inputs)\n",
    "        spk4 = outputs[3]  # Output layer spikes\n",
    "\n",
    "        loss, accuracy, predicted, targets = compute_loss_and_metrics(spk4, labels)\n",
    "        total_acc += accuracy\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_acc = total_acc / num_batches\n",
    "    print(f\"Validation accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nTutorial completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
